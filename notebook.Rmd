# TP1 - Document clustering avec le modèle JOSE sur BBC-News dataset

Ce TP est réalisé dans le cadre d'évaluation du module mixture-models et co-clustering du master 2 MLDS - Machine Learning for Data Science.

Equipe :

-   Abdesselam BENAMEUR

-   Hakim IGUENI

# Install Packages

```{r}
install.packages("NbClust")
# install.packages("factoextra")
install.packages("skmeans")
install.packages("movMF")
install.packages("R.matlab")
install.packages("aricode")
```

# Import Packages

```{r}
library(NbClust)
# library(factoextra)
library(skmeans)
library(movMF)
library(R.matlab)
library(aricode)
```

# Load Data

```{r}
# Le lien pour télécharger les deux formats de données ainsi que les labels.
url <- "https://cifre.s3.eu-north-1.amazonaws.com/bbc_dataset.mat"
# Définir le nom du fichier de destination.
# destination_file <- "bbc_dataset.mat"
# Télécharger le fichier.
# download.file(url, destfile = destination_file, method = "auto")
bbc_dataset <- readMat(url)
bbc_jose <- bbc_dataset$jose
bbc_doc_term <- bbc_dataset$doc.term
bbc_labels <- as.vector(bbc_dataset$labels)
```

```{r}
dim(bbc_jose)
```

```{r}
dim(bbc_doc_term)
```

1.  **Quelle est la dimension de chaque dataset (nombre documents, nombre de mots/taille de la dimension) ?**

-   Le 1er dataset (la matrice document-terme, la variable bbc_doc_term)

    -   nombre documents: 2225

    -   nombre de mots: 2000

-   Le 2eme dataset (la matrice des embeddings des documents, la variable bbc_jose)

    -   nombre documents: 2225

    -   taille de la dimension: 100

2.  **Exécuter tous les algorithmes de clustering avec le vrai nombre de classes qui est 5.**

3.  **Afficher les matrices de confusion obtenues à chaque application d'un algorithme de clustering.**

4.  **Pour mesurer la qualité du clustering obtenu, on s'appuiera sur les mesures externes : NMI et ARI.**

5.  **Commenter les résultats obtenus.**

6.  ***Questions optionnelles :***

    ***a. Toute utilisation d'un algorithme de clustering supplémentaire sera apprécié.***

    ***b. Une analyse sur le nombre de clusters peut être envisagée à partir de vos connaissances.***

# Clustering of the first Dataset (bbc_doc_term)

## K-means

```{r}
# we use nbclust, and k = 5
set.seed(123)
res.kmeans <- NbClust(bbc_doc_term, distance = "euclidean", min.nc = 4, max.nc = 6, method = "kmeans")
# res.kmeans <- kmeans(bbc_doc_term, 5, nstart = 25)

# # we use fviz_cluster to visualize the clusters without points labels
# library(factoextra)
# fviz_cluster(res.kmeans, data = bbc_jose, ellipse.type = "norm", 
#              palette = "jco", ggtheme = theme_minimal(),
#              main = "K-means clustering of documents")
```

```{r}
# confusion matrix
clusters.kmeans <- res.kmeans$Best.partition
table(clusters.kmeans, bbc_labels)

# NMI
nmi.kmeans <- NMI(clusters.kmeans, bbc_labels)
paste("NMI:", nmi.kmeans)

# ARI
ari.kmeans <- ARI(clusters.kmeans, bbc_labels)
paste("ARI:", ari.kmeans)
```

## Spherical K-means

```{r}
# we use skmeans, and k = 5
set.seed(123)
res.skmeans <- skmeans(bbc_doc_term, 5)

# confusion matrix
clusters.skmeans <- res.skmeans$cluster
table(clusters.skmeans, bbc_labels)

# NMI
nmi.skmeans <- NMI(clusters.skmeans, bbc_labels)
paste("NMI:", nmi.skmeans)

# ARI
ari.skmeans <- ARI(clusters.skmeans, bbc_labels)
paste("ARI:", ari.skmeans)
```

## von-Mises Fisher Mixture Model

```{r}
# we use movMF, and k = 5
set.seed(123)
res.movMF <- movMF(bbc_doc_term, 5, kappa=list(common = TRUE), nruns=5, maxit=200)
clusters.movMF <- apply(res.movMF$P,1,which.max)

# confusion matrix
table(clusters.movMF, bbc_labels)

# NMI
nmi.movMF <- NMI(clusters.movMF, bbc_labels)
paste("NMI:", nmi.movMF)

# ARI
ari.movMF <- ARI(clusters.movMF, bbc_labels)
paste("ARI:", ari.movMF)
```

## Hierarchical Clustering

```{r}
# we use hclust, and k = 5
res.hclust <- hclust(dist(bbc_doc_term), method = "ward.D2")

# confusion matrix
print("Confusion matrix")
clusters.hclust <- cutree(res.hclust, 5)
table(clusters.hclust, bbc_labels)

# NMI
nmi.hclust <- NMI(clusters.hclust, bbc_labels)
paste("NMI:", nmi.hclust)

# ARI

ari.hclust <- ARI(clusters.hclust, bbc_labels)
paste("ARI:", ari.hclust)
```

## Summary

| Clustering algorithm            | NMI                                 | ARI                                 |
|---------------------------------|-------------------------------------|-------------------------------------|
| K-means                         | 0.0628969449475587                  | 0.0633143450678484                  |
| [Spherical K-means]{.underline} | [**0.276233388100149**]{.underline} | [**0.212289829517698**]{.underline} |
| von-Mises Fisher Mixture Model  | 0.261955530184438                   | 0.191144431198362                   |
| Hierarchical Clustering         | 0.074779266469138                   | 0.0657793900465407                  |

-   The K-means algorithm shows relatively low values for both NMI and ARI. This suggests that the clusters generated by K-means do not align well with the true underlying structure of the data.

-   Spherical K-means performs better than the traditional K-means, indicating that considering the spherical nature of the data space improves clustering quality. However, there is still room for improvement.

-   The von-Mises Fisher Mixture Model demonstrates competitive performance, yielding higher NMI and ARI than the traditional K-means. It suggests that the model accounts for the distributional characteristics of the data better.

-   Hierarchical clustering shows marginal improvement compared to K-means but still exhibits relatively low values for both NMI and ARI. This indicates that the hierarchical structure might not align well with the true clusters in the data.

In summary, none of the methods seem to perform exceptionally well, with NMI and ARI scores indicating suboptimal alignment with the true underlying structure of the BBC News dataset.

# Clustering of second Dataset (bbc_jose)

## K-means

```{r}
# we use nbclust, and k = 5
set.seed(123)
res.kmeans <- NbClust(bbc_jose, distance = "euclidean", min.nc = 4, max.nc = 6, method = "kmeans")
#res.kmeans <- kmeans(bbc_jose, 5, nstart = 25)

# # we use fviz_cluster to visualize the clusters without points labels
# library(factoextra)
# fviz_cluster(res.kmeans, data = bbc_jose, ellipse.type = "norm", 
#              palette = "jco", ggtheme = theme_minimal(),
#              main = "K-means clustering of documents")


```

```{r}
# confusion matrix
clusters.kmeans <- res.kmeans$Best.partition
table(clusters.kmeans, bbc_labels)

# NMI
nmi.kmeans <- NMI(clusters.kmeans, bbc_labels)
paste("NMI:", nmi.kmeans)

# ARI
ari.kmeans <- ARI(clusters.kmeans, bbc_labels)
paste("ARI:", ari.kmeans)
```

En utilisant le package Nbclust avec plusieurs valeurs de k = {4, 5, 6} pour le nombre de clusters, on conclut que le meilleur clustering est celui avec K=5.

## Spherical K-means

```{r}
# we use skmeans, and k = 5
set.seed(123)
res.skmeans <- skmeans(bbc_jose, 5, control = list(maxiter=200))

# confusion matrix
clusters.skmeans <- res.skmeans$cluster
table(clusters.skmeans, bbc_labels)

# NMI
nmi.skmeans <- NMI(clusters.skmeans, bbc_labels)
paste("NMI:", nmi.skmeans)

# ARI
ari.skmeans <- ARI(clusters.skmeans, bbc_labels)
paste("ARI:", ari.skmeans)
```

## von-Mises Fisher Mixture Model

```{r}
# we use movMF, and k = 5
set.seed(123)
res.movMF <- movMF(bbc_jose, 5, kappa=list(common = TRUE), nruns=5, maxit=200)
clusters.movMF <- apply(res.movMF$P,1,which.max)

# confusion matrix
table(clusters.movMF, bbc_labels)

# NMI
nmi.movMF <- NMI(clusters.movMF, bbc_labels)
paste("NMI:", nmi.movMF)

# ARI
ari.movMF <- ARI(clusters.movMF, bbc_labels)
paste("ARI:", ari.movMF)
```

## Hierarchical Clustering

```{r}
# we use hclust, and k = 5
res.hclust <- hclust(dist(bbc_jose), method = "ward.D2")

# confusion matrix
print("Confusion matrix")
clusters.hclust <- cutree(res.hclust, 5)
table(clusters.hclust, bbc_labels)

# NMI
nmi.hclust <- NMI(clusters.hclust, bbc_labels)
paste("NMI:", nmi.hclust)

# ARI

ari.hclust <- ARI(clusters.hclust, bbc_labels)
paste("ARI:", ari.hclust)
```

## Summary

| Clustering algorithm           | NMI                                 | ARI                                 |
|------------------------|------------------------|------------------------|
| [K-means]{.underline}          | [**0.857278886320236**]{.underline} | [**0.893826147337719**]{.underline} |
| Spherical K-means              | 0.84766649826647                    | 0.886328685712571                   |
| von-Mises Fisher Mixture Model | 0.84511546069262                    | 0.884235720048883                   |
| Hierarchical Clustering        | 0.843993589277633                   | 0.875001096159056                   |

-   K-means, when applied with JoSE embeddings, shows significantly higher NMI and ARI compared to the traditional doc-term matrix. This indicates that the Spherical text embeddings capture the inherent structure of the data more effectively, resulting in more coherent clusters.

-   Spherical K-means continues to demonstrate strong performance with JoSE embeddings. The high NMI and ARI scores suggest that the Spherical embeddings contribute to better-defined clusters and improved alignment with the true structure of the data.

-   Similar to K-means and Spherical K-means, the von-Mises Fisher Mixture Model exhibits robust clustering performance with JoSE embeddings. The model effectively captures the spherical distribution in the text embeddings, leading to accurate clustering.

-   Hierarchical clustering with JoSE embeddings shows consistently strong results, with high NMI and ARI scores. This suggests that the hierarchical structure aligns well with the inherent clusters in the data, as captured by the Spherical text embeddings.

In summary, applying JoSE document embeddings significantly enhances the performance of clustering algorithms on the BBC News dataset. The higher NMI and ARI scores across all methods indicate that the embeddings better represent the semantic relationships within the text, leading to more accurate and cohesive clustering results.
